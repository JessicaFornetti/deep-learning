{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b250132c-afc7-4f61-ad29-5b6e448bba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries needed for preprocessing the text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6e10d",
   "metadata": {},
   "source": [
    "We load the training data which we will split into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36eca5da-5a52-46b7-8985-aed4b2831a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f7fea6c-f0df-4298-a126-5024ee3a26a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290183, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c9e14d",
   "metadata": {},
   "source": [
    "The dataset is downsized as I am running the models for the 1st task on my laptop (with only CPU), as I was not able to load the dataset correctly in Google Collab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33d3ced7-5b58-429e-9cde-d4f8dcfe8f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=int(0.2*len(df))\n",
    "df=df.sample(sample_size,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "780a3c62-11e1-4804-a661-78ce3d02db4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58036, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30684317-5bdf-413f-9db6-9d87bd7ec5bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Language</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180931</th>\n",
       "      <td>big daddy</td>\n",
       "      <td>hold on</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>I know there's pain\\nWhy do lock yourself up i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269990</th>\n",
       "      <td>aqualung</td>\n",
       "      <td>if i fall</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>swept away\\nby the wonder of it all\\nso amazed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275612</th>\n",
       "      <td>george ezra</td>\n",
       "      <td>benjamin twine</td>\n",
       "      <td>Indie</td>\n",
       "      <td>en</td>\n",
       "      <td>Let me tell you about my best friend, he got h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195014</th>\n",
       "      <td>we came as romans</td>\n",
       "      <td>what i wished i never had</td>\n",
       "      <td>Metal</td>\n",
       "      <td>en</td>\n",
       "      <td>Don't catch me at the wrong time\\nOr you will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26829</th>\n",
       "      <td>jimmy eat world</td>\n",
       "      <td>softer (she's perfect)</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>She's perfect in her own way.\\nSmoke rings ris...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Artist                       Song  Genre Language  \\\n",
       "180931          big daddy                    hold on   Rock       en   \n",
       "269990           aqualung                  if i fall   Rock       en   \n",
       "275612        george ezra             benjamin twine  Indie       en   \n",
       "195014  we came as romans  what i wished i never had  Metal       en   \n",
       "26829     jimmy eat world     softer (she's perfect)   Rock       en   \n",
       "\n",
       "                                                   Lyrics  \n",
       "180931  I know there's pain\\nWhy do lock yourself up i...  \n",
       "269990  swept away\\nby the wonder of it all\\nso amazed...  \n",
       "275612  Let me tell you about my best friend, he got h...  \n",
       "195014  Don't catch me at the wrong time\\nOr you will ...  \n",
       "26829   She's perfect in her own way.\\nSmoke rings ris...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2674c",
   "metadata": {},
   "source": [
    "We create a sub dataframe for the first part taking only the Lyrics and Genre columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "153d6765-85ad-46f0-b3d1-41189827624e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180931</th>\n",
       "      <td>Rock</td>\n",
       "      <td>I know there's pain\\nWhy do lock yourself up i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269990</th>\n",
       "      <td>Rock</td>\n",
       "      <td>swept away\\nby the wonder of it all\\nso amazed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275612</th>\n",
       "      <td>Indie</td>\n",
       "      <td>Let me tell you about my best friend, he got h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195014</th>\n",
       "      <td>Metal</td>\n",
       "      <td>Don't catch me at the wrong time\\nOr you will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26829</th>\n",
       "      <td>Rock</td>\n",
       "      <td>She's perfect in her own way.\\nSmoke rings ris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81372</th>\n",
       "      <td>Pop</td>\n",
       "      <td>[Intro: Pete Ross &amp; Lil Wayne]\\nIs it true you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94338</th>\n",
       "      <td>Rock</td>\n",
       "      <td>Ev'rything I want I got\\nAnd I got you girl\\nY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9830</th>\n",
       "      <td>Pop</td>\n",
       "      <td>Last year is old news\\nI'm breaking out my six...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33110</th>\n",
       "      <td>Rock</td>\n",
       "      <td>Você teima!\\nVocê teima!\\nVocê teima!\\nVocê te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275495</th>\n",
       "      <td>Electronic</td>\n",
       "      <td>Living on honeycomb Outside the fair still rag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58036 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Genre                                             Lyrics\n",
       "180931        Rock  I know there's pain\\nWhy do lock yourself up i...\n",
       "269990        Rock  swept away\\nby the wonder of it all\\nso amazed...\n",
       "275612       Indie  Let me tell you about my best friend, he got h...\n",
       "195014       Metal  Don't catch me at the wrong time\\nOr you will ...\n",
       "26829         Rock  She's perfect in her own way.\\nSmoke rings ris...\n",
       "...            ...                                                ...\n",
       "81372          Pop  [Intro: Pete Ross & Lil Wayne]\\nIs it true you...\n",
       "94338         Rock  Ev'rything I want I got\\nAnd I got you girl\\nY...\n",
       "9830           Pop  Last year is old news\\nI'm breaking out my six...\n",
       "33110         Rock  Você teima!\\nVocê teima!\\nVocê teima!\\nVocê te...\n",
       "275495  Electronic  Living on honeycomb Outside the fair still rag...\n",
       "\n",
       "[58036 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = df.drop([\"Artist\", \"Song\", \"Language\"], axis= 1)\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b3c401",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 58036 entries, 180931 to 275495\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Genre   58036 non-null  object\n",
      " 1   Lyrics  58027 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "sub_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f7090",
   "metadata": {},
   "source": [
    "We remove the null values in the Lyrics column from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32cc4fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e79078",
   "metadata": {},
   "source": [
    "Now we start preprocessing the text. We remove the stopwords, the punctuation and conevrt the text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ed776a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=stopwords.words(\"english\")\n",
    "sub_df[\"Lyrics\"] = sub_df[\"Lyrics\"].apply(lambda x: \" \".join(word for word in x.split() if word not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dea786c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first remove the \\n that is in the lyrics column first as it seperates all the sentences and then remove the punctuation\n",
    "sub_df[\"Lyrics\"]=sub_df[\"Lyrics\"].apply(lambda x:(re.sub(r\"\\n\",' ',str(x))))\n",
    "sub_df[\"Lyrics\"]=sub_df[\"Lyrics\"].apply(lambda x:(re.sub( r\"[^\\w\\s]\",'',str(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f154a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df[\"Lyrics\"]=sub_df[\"Lyrics\"].apply(lambda x: str(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad9f6300",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180931</th>\n",
       "      <td>Rock</td>\n",
       "      <td>i know theres pain why lock chains no one chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269990</th>\n",
       "      <td>Rock</td>\n",
       "      <td>swept away wonder amazed never saw coming left...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275612</th>\n",
       "      <td>Indie</td>\n",
       "      <td>let tell best friend got hair knees he gets al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195014</th>\n",
       "      <td>Metal</td>\n",
       "      <td>dont catch wrong time or feel wrath the one i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26829</th>\n",
       "      <td>Rock</td>\n",
       "      <td>shes perfect way smoke rings rising winter gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81372</th>\n",
       "      <td>Pop</td>\n",
       "      <td>intro pete ross  lil wayne is true performed w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94338</th>\n",
       "      <td>Rock</td>\n",
       "      <td>evrything i want i got and i got girl you real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9830</th>\n",
       "      <td>Pop</td>\n",
       "      <td>last year old news im breaking six string and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33110</th>\n",
       "      <td>Rock</td>\n",
       "      <td>você teima você teima você teima você teima e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275495</th>\n",
       "      <td>Electronic</td>\n",
       "      <td>living honeycomb outside fair still rages youn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58027 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Genre                                             Lyrics\n",
       "180931        Rock  i know theres pain why lock chains no one chan...\n",
       "269990        Rock  swept away wonder amazed never saw coming left...\n",
       "275612       Indie  let tell best friend got hair knees he gets al...\n",
       "195014       Metal  dont catch wrong time or feel wrath the one i ...\n",
       "26829         Rock  shes perfect way smoke rings rising winter gre...\n",
       "...            ...                                                ...\n",
       "81372          Pop  intro pete ross  lil wayne is true performed w...\n",
       "94338         Rock  evrything i want i got and i got girl you real...\n",
       "9830           Pop  last year old news im breaking six string and ...\n",
       "33110         Rock  você teima você teima você teima você teima e ...\n",
       "275495  Electronic  living honeycomb outside fair still rages youn...\n",
       "\n",
       "[58027 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dd453e",
   "metadata": {},
   "source": [
    "Now we split our dataset into training (80%) and validation (20%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c98a53c3-794d-48ff-9106-491def06bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val =train_test_split(sub_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdd94814-b61f-4634-8dbe-4d52987b29db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:(46421, 2)\n",
      "validation:(11606, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"train:{train.shape}\")\n",
    "print(f\"validation:{val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92abbad1",
   "metadata": {},
   "source": [
    "Now we tokenize our lyrics, to feed that input into our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "981d9655-5332-4658-af4d-06dab5da5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take only the 20000 most frequently occuring words\n",
    "max_nb_words=20000\n",
    "tokenizer= Tokenizer(num_words=max_nb_words)\n",
    "tokenizer.fit_on_texts(train.Lyrics)# we fit the tokenizer on the train set only to avoid data leakage\n",
    "train_sequences = tokenizer.texts_to_sequences(train.Lyrics)# and tokenize both the train and validation set\n",
    "val_sequences = tokenizer.texts_to_sequences(val.Lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d68e0e",
   "metadata": {},
   "source": [
    "To make sure our tokenized sequences are of equal length we use pad the sequences to be of maximum length 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db2d47bc-2024-474d-a422-2ad069cb17fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46421, 300)\n",
      "(11606, 300)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH=300\n",
    "train_data=pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "val_data = pad_sequences(val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac57739",
   "metadata": {},
   "source": [
    "Now we save the labels in a seperate dataframe, which we will encode using the LabelEncoder to have numerical values (instead of the text labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46680248",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train[\"Genre\"]\n",
    "val_labels = val[\"Genre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c34c9543-ae24-41fa-afdb-5c252a099247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Country' 'Electronic' 'Folk' 'Hip-Hop' 'Indie' 'Jazz' 'Metal' 'Pop'\n",
      " 'R&B' 'Rock']\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([  318,   336,  1398,   363,  1391,  2215,  3236, 17311,   409,\n",
      "       19444], dtype=int64))\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([  81,   91,  335,   94,  343,  558,  823, 4260,  107, 4914],\n",
      "      dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "le= LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "\n",
    "train_labels=le.transform(train_labels)\n",
    "val_labels=le.transform(val_labels)\n",
    "\n",
    "print(le.classes_)\n",
    "print(np.unique(train_labels, return_counts=True))\n",
    "print(np.unique(val_labels, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d46401",
   "metadata": {},
   "source": [
    "We can see the 10 classes, which are unbalanced with Pop having by far the most instances.\n",
    "\n",
    "We convert the nurical labels into one-hot encoded vectors to feed into our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bf9de57-1331-4ceb-9607-10d848972154",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (46421, 300)\n",
      "Shape of label tensor: (46421, 10)\n",
      "Shape of label tensor: (11606, 10)\n"
     ]
    }
   ],
   "source": [
    "labels_train = to_categorical(np.asarray(train_labels))\n",
    "labels_val = to_categorical(np.asarray(val_labels))\n",
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', labels_train.shape)\n",
    "print('Shape of label tensor:', labels_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b6f90c",
   "metadata": {},
   "source": [
    "## Models based only on Lyrics\n",
    "Now we we start training the models based on the lyrics only\n",
    "\n",
    "### RNN Variants\n",
    "#### Basic RNN Model:\n",
    "First we start with a Simple RNN model.\n",
    "\n",
    "We import the tensorflow libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4072ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, SimpleRNN, LSTM, Activation, Flatten, Embedding, Activation, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, MaxPooling1D, Input, Concatenate, RepeatVector, Reshape\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df009a",
   "metadata": {},
   "source": [
    "We define the batch size of 1000 to speed up training and use a state size of 10 for the RNN and LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "242f9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "state_size = 10\n",
    "num_classes = 10 #there are 10 genre classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798128af",
   "metadata": {},
   "source": [
    "For the simple RNN model:\n",
    "1) We use on the fly embeddings of size 100 for our tokenized text\n",
    "2) We use a SimpleRNN layer, giving the state size of 10\n",
    "2) We use Batch Normalization and Dropout of 20% to prevent overfitting as Batch Normalization helps to stabilize and speed up the training process by normalizing the activations and Dropout encourages a sparser network reliant on more independant neurons. \n",
    "3) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "4) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "5) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "6) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "7) We fit the model on the training data, providing the batch size, number of epochs, the validation data and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9943a9b6-cad0-4fa5-b869-d9fb7cdc7b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_41 (Embedding)    (None, 300, 100)          2000000   \n",
      "                                                                 \n",
      " simple_rnn_10 (SimpleRNN)   (None, 10)                1110      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 10)                40        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2001260 (7.63 MB)\n",
      "Trainable params: 2001240 (7.63 MB)\n",
      "Non-trainable params: 20 (80.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "47/47 [==============================] - 20s 352ms/step - loss: 2.3301 - accuracy: 0.2815 - val_loss: 2.0507 - val_accuracy: 0.4937\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 18s 385ms/step - loss: 1.8658 - accuracy: 0.5175 - val_loss: 1.9247 - val_accuracy: 0.5485\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 22s 470ms/step - loss: 1.6507 - accuracy: 0.5695 - val_loss: 1.8637 - val_accuracy: 0.4989\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 20s 431ms/step - loss: 1.5378 - accuracy: 0.5758 - val_loss: 1.7866 - val_accuracy: 0.5061\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 19s 415ms/step - loss: 1.2963 - accuracy: 0.6479 - val_loss: 1.6998 - val_accuracy: 0.5090\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 17s 358ms/step - loss: 1.1386 - accuracy: 0.6884 - val_loss: 1.6692 - val_accuracy: 0.5048\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 19s 405ms/step - loss: 1.0239 - accuracy: 0.7145 - val_loss: 1.6766 - val_accuracy: 0.5013\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 22s 455ms/step - loss: 0.9437 - accuracy: 0.7303 - val_loss: 1.7196 - val_accuracy: 0.4972\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 17s 365ms/step - loss: 0.8773 - accuracy: 0.7465 - val_loss: 1.7824 - val_accuracy: 0.4928\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 18s 392ms/step - loss: 0.8307 - accuracy: 0.7579 - val_loss: 1.8644 - val_accuracy: 0.4898\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 21s 448ms/step - loss: 0.7900 - accuracy: 0.7627 - val_loss: 1.9407 - val_accuracy: 0.4860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2566bdcca90>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_nb_words, 100, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(SimpleRNN(state_size))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                   optimizer='adam',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(val_data, labels_val),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4003eda",
   "metadata": {},
   "source": [
    "#### Single Layer LSTM Model:\n",
    "Similarly for the signle layer LSTM model:\n",
    "1) We use on the fly embeddings of size 100 for our tokenized text\n",
    "2) For the LSTM layer, we set the state size and return_sequences to True to output all the hidden states for each timestep of the text sequence. The we flatten the output to pass to the Batch Normalization layer.\n",
    "2) We use Batch Normalization and Dropout of 20% to prevent overfitting as Batch Normalization helps to stabilize and speed up the training process by normalizing the activations and Dropout encourages a sparser network reliant on more independant neurons. \n",
    "3) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "4) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "5) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "6) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "7) We fit the model on the training data, providing the batch size, number of epochs, the validation data and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b398e0a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_nb_words, 100, input_length= MAX_SEQUENCE_LENGTH))\n",
    "model.add(LSTM(state_size, return_sequences=True))  # return_sequences True as we want to feed the hidden state inputs to our following layers\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(val_data, labels_val),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f3bf92",
   "metadata": {},
   "source": [
    "#### Multi-Layer LSTM Model:\n",
    "Similarly for the multi layer LSTM model:\n",
    "1) We use on the fly embeddings of size 100 for our tokenized text\n",
    "2) For the LSTM layer, we set the state size to 10 and return_sequences to True to output all the hidden states for each timestep of the text sequence. The we flatten the output to pass to the Batch Normalization layer.\n",
    "2) We use Batch Normalization and Dropout of 20% to prevent overfitting as Batch Normalization helps to stabilize and speed up the training process by normalizing the activations and Dropout encourages a sparser network reliant on more independant neurons. \n",
    "3) Then we add the 2nd LSTM layer with a state size of 10.\n",
    "3) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "4) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "5) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "6) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "7) We fit the model on the training data, providing the batch size, number of epochs, the validation data and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "89985b29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jess6\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 1s/step - accuracy: 0.2971 - loss: 1.9919 - val_accuracy: 0.4172 - val_loss: 1.9942\n",
      "Epoch 2/20\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - accuracy: 0.4634 - loss: 1.5367 - val_accuracy: 0.5049 - val_loss: 1.6462\n",
      "Epoch 3/20\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - accuracy: 0.5274 - loss: 1.3789 - val_accuracy: 0.4416 - val_loss: 1.5004\n",
      "Epoch 4/20\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - accuracy: 0.5593 - loss: 1.2928 - val_accuracy: 0.4336 - val_loss: 1.4496\n",
      "Epoch 5/20\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - accuracy: 0.5928 - loss: 1.2093 - val_accuracy: 0.4331 - val_loss: 1.4401\n",
      "Epoch 6/20\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 1s/step - accuracy: 0.6351 - loss: 1.1263 - val_accuracy: 0.4506 - val_loss: 1.4478\n",
      "Epoch 7/20\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - accuracy: 0.6626 - loss: 1.0709 - val_accuracy: 0.5161 - val_loss: 1.3762\n",
      "Epoch 8/20\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - accuracy: 0.6870 - loss: 1.0097 - val_accuracy: 0.5280 - val_loss: 1.4013\n",
      "Epoch 9/20\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - accuracy: 0.7043 - loss: 0.9634 - val_accuracy: 0.5520 - val_loss: 1.3496\n",
      "Epoch 10/20\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - accuracy: 0.7190 - loss: 0.9121 - val_accuracy: 0.5488 - val_loss: 1.3842\n",
      "Epoch 11/20\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 2s/step - accuracy: 0.7348 - loss: 0.8715 - val_accuracy: 0.5588 - val_loss: 1.3507\n",
      "Epoch 12/20\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 2s/step - accuracy: 0.7494 - loss: 0.8273 - val_accuracy: 0.5569 - val_loss: 1.3808\n",
      "Epoch 13/20\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 2s/step - accuracy: 0.7596 - loss: 0.7953 - val_accuracy: 0.5604 - val_loss: 1.3951\n",
      "Epoch 14/20\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 2s/step - accuracy: 0.7645 - loss: 0.7747 - val_accuracy: 0.5600 - val_loss: 1.4135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x167ed4dfa10>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_nb_words, 100, input_length= MAX_SEQUENCE_LENGTH))\n",
    "model.add(LSTM(10, return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(num_classes, activation=tf.nn.softmax))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                   optimizer='adam',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(val_data,labels_val),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4ae144",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "Now we move on to testing the differnet embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e7077c",
   "metadata": {},
   "source": [
    "#### On the fly embeddings\n",
    "For the on the fly embeddings model we take a similar structure that we will also use for the pretrained embeddings:\n",
    "1) We use on the fly embeddings of size 100 for our tokenized text\n",
    "2) We use GlobalAveragePooling1D to reduce the dimensionality of the embeddings to feed the output to the following Dense layers.\n",
    "2) We use Dropout of 20% to prevent overfitting as it encourages a sparser network reliant on more independant neurons. \n",
    "3) Then we use a dense layer of 64 connected neurons with the relu activation function\n",
    "3) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "4) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "5) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "6) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "7) We fit the model on the training data, providing the batch size, number of epochs, the validation data and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d8bb9a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jess6\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 25ms/step - accuracy: 0.5050 - loss: 1.4378 - val_accuracy: 0.5800 - val_loss: 1.2244\n",
      "Epoch 2/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 31ms/step - accuracy: 0.5842 - loss: 1.2111 - val_accuracy: 0.5902 - val_loss: 1.1610\n",
      "Epoch 3/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 29ms/step - accuracy: 0.6283 - loss: 1.0943 - val_accuracy: 0.6173 - val_loss: 1.1213\n",
      "Epoch 4/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 25ms/step - accuracy: 0.6481 - loss: 1.0411 - val_accuracy: 0.6184 - val_loss: 1.1158\n",
      "Epoch 5/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 27ms/step - accuracy: 0.6702 - loss: 0.9649 - val_accuracy: 0.6229 - val_loss: 1.1224\n",
      "Epoch 6/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 27ms/step - accuracy: 0.6869 - loss: 0.9178 - val_accuracy: 0.6189 - val_loss: 1.1345\n",
      "Epoch 7/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.6989 - loss: 0.8789 - val_accuracy: 0.6143 - val_loss: 1.1555\n",
      "Epoch 8/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 28ms/step - accuracy: 0.7088 - loss: 0.8379 - val_accuracy: 0.6132 - val_loss: 1.1758\n",
      "Epoch 9/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 30ms/step - accuracy: 0.7260 - loss: 0.7919 - val_accuracy: 0.6146 - val_loss: 1.2165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e406865bd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(max_nb_words, 100, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit(train_data, labels_train, epochs=20, validation_data=(val_data, labels_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229772f",
   "metadata": {},
   "source": [
    "We save this model as one of our 2 best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "save_model(model, 'on_the_fly_embeddings_dropout_only_lyrics.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe866dde",
   "metadata": {},
   "source": [
    "#### Pre trained embeddings\n",
    "For the pretrained embeddings, we load a pretrained model from tensorflow hub that gives embeddings in 50 dimensions, which we then train on our training and validation lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3b9c4736",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jess6\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jess6\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jess6\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jess6\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jess6\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jess6\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jess6\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "embeding_model = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
    "embeddings_train = embeding_model(train.Lyrics)\n",
    "embeddings_val = embeding_model(val.Lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01278f7",
   "metadata": {},
   "source": [
    "For the pre trained embeddings model we use a similar structure:\n",
    "1) We use a dense layer of 64 connected neurons with the relu activation function\n",
    "2) We use Dropout of 20% to prevent overfitting as it encourages a sparser network reliant on more independant neurons.\n",
    "3) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "4) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "5) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "6) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "7) We fit the model on the training data that has been embedded using the pre trained embeddings, providing the batch size, number of epochs, the validation data that has also been embedded using the pre trained embeddings and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9008274f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jess6\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.5003 - loss: 1.3976 - val_accuracy: 0.5523 - val_loss: 1.2601\n",
      "Epoch 2/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.5480 - loss: 1.2493 - val_accuracy: 0.5564 - val_loss: 1.2413\n",
      "Epoch 3/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.5507 - loss: 1.2294 - val_accuracy: 0.5613 - val_loss: 1.2269\n",
      "Epoch 4/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.5531 - loss: 1.2232 - val_accuracy: 0.5637 - val_loss: 1.2212\n",
      "Epoch 5/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.5563 - loss: 1.2202 - val_accuracy: 0.5656 - val_loss: 1.2198\n",
      "Epoch 6/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.5606 - loss: 1.2096 - val_accuracy: 0.5663 - val_loss: 1.2118\n",
      "Epoch 7/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.5638 - loss: 1.2015 - val_accuracy: 0.5670 - val_loss: 1.2099\n",
      "Epoch 8/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.5644 - loss: 1.2000 - val_accuracy: 0.5727 - val_loss: 1.2080\n",
      "Epoch 9/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.5652 - loss: 1.1979 - val_accuracy: 0.5672 - val_loss: 1.2082\n",
      "Epoch 10/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.5669 - loss: 1.1829 - val_accuracy: 0.5688 - val_loss: 1.2126\n",
      "Epoch 11/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.5650 - loss: 1.1955 - val_accuracy: 0.5709 - val_loss: 1.2045\n",
      "Epoch 12/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.5689 - loss: 1.1867 - val_accuracy: 0.5703 - val_loss: 1.2051\n",
      "Epoch 13/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5639 - loss: 1.1907 - val_accuracy: 0.5702 - val_loss: 1.2014\n",
      "Epoch 14/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.5674 - loss: 1.1794 - val_accuracy: 0.5680 - val_loss: 1.2002\n",
      "Epoch 15/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5737 - loss: 1.1752 - val_accuracy: 0.5671 - val_loss: 1.2033\n",
      "Epoch 16/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.5734 - loss: 1.1739 - val_accuracy: 0.5715 - val_loss: 1.2019\n",
      "Epoch 17/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.5706 - loss: 1.1744 - val_accuracy: 0.5710 - val_loss: 1.1990\n",
      "Epoch 18/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.5743 - loss: 1.1756 - val_accuracy: 0.5699 - val_loss: 1.2037\n",
      "Epoch 19/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5714 - loss: 1.1765 - val_accuracy: 0.5725 - val_loss: 1.2020\n",
      "Epoch 20/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5731 - loss: 1.1742 - val_accuracy: 0.5709 - val_loss: 1.1999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x167a08661d0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(50,)),\n",
    "    Dropout(0.2),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit(embeddings_train, labels_train, \n",
    "          epochs=20, \n",
    "          validation_data=(embeddings_val, labels_val), \n",
    "          callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453554a",
   "metadata": {},
   "source": [
    "We also save this 2nd model as it is the model with the least overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9e6dfedf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "save_model(model, 'pretrained_embeddings_only_lyrics.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e746c9b",
   "metadata": {},
   "source": [
    "#### Traditional text encoding approach\n",
    "\n",
    "Now we compare the previous models to a more traditional text approach:\n",
    "\n",
    "We use TF-IDF to transform the text into a numerical representation. Then we feed that embedding to a Logistic Regression Model used for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1413855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Accuracy: 0.6068412889884542\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000) #we define the TF-IDFVectorizer using a maximum features of 5000\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train.Lyrics)#we fit and transform the training and validation lyrics\n",
    "val_tfidf = tfidf_vectorizer.transform(val.Lyrics)\n",
    "\n",
    "clf_tfidf = LogisticRegression(max_iter=1000) #we use the Logistic Regression Model and set the max-iter parameter to speed up training\n",
    "clf_tfidf.fit(train_tfidf, train_labels)\n",
    "preds_tfidf = clf_tfidf.predict(val_tfidf)#we make predictions on our validation set\n",
    "accuracy_tfidf = accuracy_score(val_labels, preds_tfidf)#we calculate the accuracy of those predictions\n",
    "print(f\"TF-IDF Accuracy: {accuracy_tfidf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d52ea9a",
   "metadata": {},
   "source": [
    "### CNN for Text Classification\n",
    "\n",
    "#### CNNs with same kernel sizes:\n",
    "For the CNN with same kernel size model:\n",
    "1) We use on the fly embeddings of size 100 for our tokenized text\n",
    "2) Then we use a 1 Dimension Convolutional layer, with 25 different filters, a kernel size of 3, no additional padding and a stride of 1 to capture more local features\n",
    "3) Then we use Batch Normalization to prevent overfitting as it helps to stabilize and speed up the training process by normalizing the data.\n",
    "4) We use the relu activation function, followed by Max Pooling to reduce the size of our feature maps\n",
    "5) We use Dropout of 20% to prevent overfitting as it encourages a sparser network reliant on more independant neurons. \n",
    "6) We repeat this structure to have 3 blocks of convolutional features.\n",
    "7) Then we flatten the output and feed it to a dense layer of 50 connected neurons using the relu activation function\n",
    "8) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "9) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "10) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "11) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "12) We fit the model on the training data, providing the batch size, number of epochs, the validation data and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffe09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_nb_words, 100, input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "model.add(Conv1D(filters=25, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(filters=25, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(filters=30, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                   optimizer='adam',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(val_data,labels_val),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746a5a9c",
   "metadata": {},
   "source": [
    "#### CNNs with different kernel sizes:\n",
    "For the CNN with different kernel size model, we have a nearly identical architecture. We just vary the kernel size for each of the convolutional layers, starting from 3 to 5, to capture more general features in the further layers:\n",
    "1) We use on the fly embeddings of size 100 for our tokenized text\n",
    "2) Then we use a 1 Dimension Convolutional layer, with 25 different filters, a kernel size of 3, no additional padding and a stride of 1 to capture more local features\n",
    "3) Then we use Batch Normalization to prevent overfitting as it helps to stabilize and speed up the training process by normalizing the data.\n",
    "4) We use the relu activation function, followed by Max Pooling to reduce the size of our feature maps\n",
    "5) We use Dropout of 20% to prevent overfitting as it encourages a sparser network reliant on more independant neurons. \n",
    "6) We repeat this structure to have 3 blocks of convolutional features, varying the kernel sizes going from 3 to 5, to capture more general features in the further layers\n",
    "7) Then we flatten the output and feed it to a dense layer of 50 connected neurons using the relu activation function\n",
    "8) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "9) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "10) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "11) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "12) We fit the model on the training data, providing the batch size, number of epochs, the validation data and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6181f89e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_nb_words, 100, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Conv1D(filters=25, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(filters=25, kernel_size=4, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Dropout(0.2))  \n",
    "\n",
    "model.add(Conv1D(filters=30, kernel_size=5, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Flatten()) \n",
    "\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(num_classes, activation=tf.nn.softmax))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                   optimizer='adam',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(val_data,labels_val),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f098558",
   "metadata": {},
   "source": [
    "#### CNN as an additional layer before a LSTM solution:\n",
    "For the CNN was an additional layer before a LSTM solution, we keep the same kernel size as that performs better than the differnet kernel sizes. We try 2 different versions, the first being:\n",
    "1) We use on the fly embeddings of size 100 for our tokenized text\n",
    "2) Then we use a 1 Dimension Convolutional layer, with 25 different filters, a kernel size of 3, no additional padding and a stride of 1 to capture more local features\n",
    "3) Then we use Batch Normalization to prevent overfitting as it helps to stabilize and speed up the training process by normalizing the data.\n",
    "4) We use the relu activation function, followed by Max Pooling to reduce the size of our feature maps\n",
    "5) We use Dropout of 20% to prevent overfitting as it encourages a sparser network reliant on more independant neurons. \n",
    "6) We repeat this structure to have 3 blocks of convolutional features\n",
    "7) Then we add the LSTM model with a state size of 10, followed by Batch Normalization and Dropout to prevent overfitting\n",
    "8) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "9) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "10) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "11) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "12) We fit the model on the training data, providing the batch size, number of epochs, the validation data and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aa1429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1st version\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_nb_words, 100, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Conv1D(filters=25, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(filters=25, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Dropout(0.2)) \n",
    "\n",
    "model.add(Conv1D(filters=30, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Dropout(0.2)) \n",
    "\n",
    "model.add(LSTM(10))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation=tf.nn.softmax))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                   optimizer='adam',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(val_data,labels_val),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd175a6",
   "metadata": {},
   "source": [
    "For the second version:\n",
    "1) We use on the fly embeddings of size 100 for our tokenized text\n",
    "2) Then we use a 1 Dimension Convolutional layer, with 25 different filters, a kernel size of 3, no additional padding and a stride of 1 to capture more local features\n",
    "3) Then we use Batch Normalization to prevent overfitting as it helps to stabilize and speed up the training process by normalizing the data.\n",
    "4) We use the relu activation function, followed by Max Pooling to reduce the size of our feature maps\n",
    "5) We use Dropout of 20% to prevent overfitting as it encourages a sparser network reliant on more independant neurons. \n",
    "6) We repeat this structure to have 3 blocks of convolutional features\n",
    "7) Then we flatten and resahpe the output to pass it to the LSTM model with a state size of 10, followed by Batch Normalization and Dropout to prevent overfitting\n",
    "8) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "9) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "10) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "11) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "12) We fit the model on the training data, providing the batch size, number of epochs, the validation data and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f82c2bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2nd version\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_nb_words, 100, input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "model.add(Conv1D(filters=25, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(filters=25, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(filters=30, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Reshape((1050, 1)))\n",
    "\n",
    "model.add(LSTM(10))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2)) \n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(val_data,labels_val),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a612480",
   "metadata": {},
   "source": [
    "#### Comparison to Non-Neural Methods\n",
    "\n",
    "Now we compare to traditional ML models. We use the Decision Tree Classifier and perform a grid search to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "946a7a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [4, 8],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum samples required for each leaf node\n",
    "}\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')#we run a 5 fold cross validation grid search to find the best hyperparameters\n",
    "\n",
    "grid_search.fit(train_data, labels_train)\n",
    "\n",
    "predictions = grid_search.predict(val_data)#we make the predictions on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633d6d5a",
   "metadata": {},
   "source": [
    "We get the accuracy of our best model on the validation set, the best hyperparameters and the best score obtained on our trainset during the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b519bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34654489057384114\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(labels_val, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "416ccf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 8, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41559cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23485c57",
   "metadata": {},
   "source": [
    "### Models based on Lyrics and Artist\n",
    "\n",
    "Now we will use the Lyrics and the Artist to predict the song genre. We repeat the similar preprocessing steps as previously, adding the Artist information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b8fe560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180931</th>\n",
       "      <td>big daddy</td>\n",
       "      <td>Rock</td>\n",
       "      <td>I know there's pain\\nWhy do lock yourself up i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269990</th>\n",
       "      <td>aqualung</td>\n",
       "      <td>Rock</td>\n",
       "      <td>swept away\\nby the wonder of it all\\nso amazed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275612</th>\n",
       "      <td>george ezra</td>\n",
       "      <td>Indie</td>\n",
       "      <td>Let me tell you about my best friend, he got h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195014</th>\n",
       "      <td>we came as romans</td>\n",
       "      <td>Metal</td>\n",
       "      <td>Don't catch me at the wrong time\\nOr you will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26829</th>\n",
       "      <td>jimmy eat world</td>\n",
       "      <td>Rock</td>\n",
       "      <td>She's perfect in her own way.\\nSmoke rings ris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81372</th>\n",
       "      <td>lil wayne</td>\n",
       "      <td>Pop</td>\n",
       "      <td>[Intro: Pete Ross &amp; Lil Wayne]\\nIs it true you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94338</th>\n",
       "      <td>bee gees</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Ev'rything I want I got\\nAnd I got you girl\\nY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9830</th>\n",
       "      <td>camp rock</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Last year is old news\\nI'm breaking out my six...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33110</th>\n",
       "      <td>lulu santos</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Você teima!\\nVocê teima!\\nVocê teima!\\nVocê te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275495</th>\n",
       "      <td>everything but the girl</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>Living on honeycomb Outside the fair still rag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58036 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Artist       Genre  \\\n",
       "180931                big daddy        Rock   \n",
       "269990                 aqualung        Rock   \n",
       "275612              george ezra       Indie   \n",
       "195014        we came as romans       Metal   \n",
       "26829           jimmy eat world        Rock   \n",
       "...                         ...         ...   \n",
       "81372                 lil wayne         Pop   \n",
       "94338                  bee gees        Rock   \n",
       "9830                  camp rock         Pop   \n",
       "33110               lulu santos        Rock   \n",
       "275495  everything but the girl  Electronic   \n",
       "\n",
       "                                                   Lyrics  \n",
       "180931  I know there's pain\\nWhy do lock yourself up i...  \n",
       "269990  swept away\\nby the wonder of it all\\nso amazed...  \n",
       "275612  Let me tell you about my best friend, he got h...  \n",
       "195014  Don't catch me at the wrong time\\nOr you will ...  \n",
       "26829   She's perfect in her own way.\\nSmoke rings ris...  \n",
       "...                                                   ...  \n",
       "81372   [Intro: Pete Ross & Lil Wayne]\\nIs it true you...  \n",
       "94338   Ev'rything I want I got\\nAnd I got you girl\\nY...  \n",
       "9830    Last year is old news\\nI'm breaking out my six...  \n",
       "33110   Você teima!\\nVocê teima!\\nVocê teima!\\nVocê te...  \n",
       "275495  Living on honeycomb Outside the fair still rag...  \n",
       "\n",
       "[58036 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df2 = df.drop([\"Song\", \"Language\"], axis= 1)\n",
    "sub_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "958c10f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 58036 entries, 180931 to 275495\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Artist  58036 non-null  object\n",
      " 1   Genre   58036 non-null  object\n",
      " 2   Lyrics  58027 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "sub_df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0486ac5b",
   "metadata": {},
   "source": [
    "We remove Nas in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bb4e44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df2.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc3ef6",
   "metadata": {},
   "source": [
    "We preprocess the text by removing stop words, punctuation and converting the text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26782bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=stopwords.words(\"english\")\n",
    "sub_df2[\"Lyrics\"] = sub_df2[\"Lyrics\"].apply(lambda x: \" \".join(word for word in x.split() if word not in stop))\n",
    "sub_df2[\"Artist\"] = sub_df2[\"Artist\"].apply(lambda x: \" \".join(word for word in x.split() if word not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9b7f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df2[\"Lyrics\"]=sub_df2[\"Lyrics\"].apply(lambda x:(re.sub(r\"\\n\",' ',str(x))))\n",
    "sub_df2[\"Lyrics\"]=sub_df2[\"Lyrics\"].apply(lambda x:(re.sub( r\"[^\\w\\s]\",'',str(x))))\n",
    "sub_df2[\"Artist\"]=sub_df2[\"Artist\"].apply(lambda x:(re.sub( r\"[^\\w\\s]\",'',str(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7c7b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df2[\"Lyrics\"]=sub_df2[\"Lyrics\"].apply(lambda x: str(x).lower())\n",
    "sub_df2[\"Artist\"]=sub_df2[\"Artist\"].apply(lambda x: str(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d49d2b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180931</th>\n",
       "      <td>big daddy</td>\n",
       "      <td>Rock</td>\n",
       "      <td>i know theres pain why lock chains no one chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269990</th>\n",
       "      <td>aqualung</td>\n",
       "      <td>Rock</td>\n",
       "      <td>swept away wonder amazed never saw coming left...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275612</th>\n",
       "      <td>george ezra</td>\n",
       "      <td>Indie</td>\n",
       "      <td>let tell best friend got hair knees he gets al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195014</th>\n",
       "      <td>came romans</td>\n",
       "      <td>Metal</td>\n",
       "      <td>dont catch wrong time or feel wrath the one i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26829</th>\n",
       "      <td>jimmy eat world</td>\n",
       "      <td>Rock</td>\n",
       "      <td>shes perfect way smoke rings rising winter gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81372</th>\n",
       "      <td>lil wayne</td>\n",
       "      <td>Pop</td>\n",
       "      <td>intro pete ross  lil wayne is true performed w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94338</th>\n",
       "      <td>bee gees</td>\n",
       "      <td>Rock</td>\n",
       "      <td>evrything i want i got and i got girl you real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9830</th>\n",
       "      <td>camp rock</td>\n",
       "      <td>Pop</td>\n",
       "      <td>last year old news im breaking six string and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33110</th>\n",
       "      <td>lulu santos</td>\n",
       "      <td>Rock</td>\n",
       "      <td>você teima você teima você teima você teima e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275495</th>\n",
       "      <td>everything girl</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>living honeycomb outside fair still rages youn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58027 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Artist       Genre  \\\n",
       "180931        big daddy        Rock   \n",
       "269990         aqualung        Rock   \n",
       "275612      george ezra       Indie   \n",
       "195014      came romans       Metal   \n",
       "26829   jimmy eat world        Rock   \n",
       "...                 ...         ...   \n",
       "81372         lil wayne         Pop   \n",
       "94338          bee gees        Rock   \n",
       "9830          camp rock         Pop   \n",
       "33110       lulu santos        Rock   \n",
       "275495  everything girl  Electronic   \n",
       "\n",
       "                                                   Lyrics  \n",
       "180931  i know theres pain why lock chains no one chan...  \n",
       "269990  swept away wonder amazed never saw coming left...  \n",
       "275612  let tell best friend got hair knees he gets al...  \n",
       "195014  dont catch wrong time or feel wrath the one i ...  \n",
       "26829   shes perfect way smoke rings rising winter gre...  \n",
       "...                                                   ...  \n",
       "81372   intro pete ross  lil wayne is true performed w...  \n",
       "94338   evrything i want i got and i got girl you real...  \n",
       "9830    last year old news im breaking six string and ...  \n",
       "33110   você teima você teima você teima você teima e ...  \n",
       "275495  living honeycomb outside fair still rages youn...  \n",
       "\n",
       "[58027 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f5c77",
   "metadata": {},
   "source": [
    "We split the dataset into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c9b3131",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2, val2 =train_test_split(sub_df2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cbf6ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:(46421, 3)\n",
      "validation:(11606, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"train:{train2.shape}\")\n",
    "print(f\"validation:{val2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdff968",
   "metadata": {},
   "source": [
    "We tokenize the lyrics for the train and validation and add padding to the sequences to ensure they are all the same legth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a66bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nb_words=20000\n",
    "tokenizer= Tokenizer(num_words=max_nb_words)\n",
    "tokenizer.fit_on_texts(train2.Lyrics)\n",
    "train_sequences2 = tokenizer.texts_to_sequences(train.Lyrics)\n",
    "val_sequences2 = tokenizer.texts_to_sequences(val.Lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62939de7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46421, 300)\n",
      "(11606, 300)\n"
     ]
    }
   ],
   "source": [
    "train_data2=pad_sequences(train_sequences2, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "val_data2 = pad_sequences(val_sequences2, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "print(train_data2.shape)\n",
    "print(val_data2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd0e27",
   "metadata": {},
   "source": [
    "We encode the labels and then apply one hot encoding to get the one hot encoded vectors given to the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30c7ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels2 = train2[\"Genre\"]\n",
    "val_labels2 = val2[\"Genre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9502b612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Country' 'Electronic' 'Folk' 'Hip-Hop' 'Indie' 'Jazz' 'Metal' 'Pop'\n",
      " 'R&B' 'Rock']\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([  333,   341,  1370,   365,  1385,  2230,  3206, 17346,   412,\n",
      "       19433], dtype=int64))\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([  66,   86,  363,   92,  349,  543,  853, 4225,  104, 4925],\n",
      "      dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "le= LabelEncoder()\n",
    "le.fit(train_labels2)\n",
    "\n",
    "train_labels2=le.transform(train_labels2)\n",
    "val_labels2=le.transform(val_labels2)\n",
    "\n",
    "print(le.classes_)\n",
    "print(np.unique(train_labels2, return_counts=True))\n",
    "print(np.unique(val_labels2, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81cfd442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (46421, 300)\n",
      "Shape of label tensor: (46421, 10)\n",
      "Shape of label tensor: (11606, 10)\n"
     ]
    }
   ],
   "source": [
    "labels_train2 = to_categorical(np.asarray(train_labels2))\n",
    "labels_val2 = to_categorical(np.asarray(val_labels2))\n",
    "\n",
    "print('Shape of data tensor:', train_data2.shape)\n",
    "print('Shape of label tensor:', labels_train2.shape)\n",
    "print('Shape of label tensor:', labels_val2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23442b07",
   "metadata": {},
   "source": [
    "Now for the Artist feature, we manually encode the artist to a numerical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc38e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "artist_name_mapping = defaultdict(int)\n",
    "artist_count = 0\n",
    "\n",
    "for name in train2.Artist: #we save the Artist count in the dictionary\n",
    "    if name not in artist_name_mapping:\n",
    "        artist_name_mapping[name] = artist_count\n",
    "        artist_count += 1\n",
    "\n",
    "\n",
    "def encode_artist_names(artist_list, artist_name_mapping):\n",
    "    encoded_artists = []\n",
    "    for name in artist_list: #for each artist in the list encode it by assigning the count value\n",
    "        encoded_artists.append(artist_name_mapping.get(name, -1))  # -1 if we have an unknown artists\n",
    "    return encoded_artists\n",
    "\n",
    "train_artist_encoded = encode_artist_names(train2.Artist, artist_name_mapping)\n",
    "val_artist_encoded = encode_artist_names(val2.Artist, artist_name_mapping)\n",
    "train_artist_encoded = np.asarray(train_artist_encoded).reshape(-1, 1)#convert to np.array and reshape to pass as input to our models\n",
    "val_artist_encoded = np.asarray(val_artist_encoded).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b22b9bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46421, 1)\n",
      "(11606, 1)\n",
      "(46421, 300)\n",
      "(11606, 300)\n"
     ]
    }
   ],
   "source": [
    "print(train_artist_encoded.shape)\n",
    "print(val_artist_encoded.shape)\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8e295",
   "metadata": {},
   "source": [
    "### RNN Variants\n",
    "#### Basic RNN Model:\n",
    "\n",
    "For the simple RNN model:\n",
    "1) We use on the fly embeddings of size 100 for our tokenized lyrics\n",
    "2) Then we define the input layer for the Artist information and repeat it 300 times to match the lyric embeddings length. Then we reshape the artist input and finally concatenate it with the on the fly embeddings for the lyrics\n",
    "2) We then use a SimpleRNN layer, giving the state size of 10\n",
    "2) We use Batch Normalization and Dropout of 20% to prevent overfitting as Batch Normalization helps to stabilize and speed up the training process by normalizing the activations and Dropout encourages a sparser network reliant on more independant neurons. \n",
    "3) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "4) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "5) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "6) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "7) We fit the model on the training data(the tokenized lyrics and the encoded artist), providing the batch size, number of epochs, the validation data((the tokenized lyrics and the encoded artist)) and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "316c015a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jess6\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jess6\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " embedding_input (InputLaye  [(None, 300)]                0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 300, 100)             2000000   ['embedding_input[0][0]']     \n",
      "                                                                                                  \n",
      " simple_rnn (SimpleRNN)      (None, 10)                   1110      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 10)                   40        ['simple_rnn[0][0]']          \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 10)                   0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " artist_input (InputLayer)   [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 10)                   110       ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2001260 (7.63 MB)\n",
      "Trainable params: 2001240 (7.63 MB)\n",
      "Non-trainable params: 20 (80.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\jess6\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jess6\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "47/47 [==============================] - 28s 482ms/step - loss: 2.4004 - accuracy: 0.1012 - val_loss: 2.1920 - val_accuracy: 0.0707\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 21s 454ms/step - loss: 2.1419 - accuracy: 0.3784 - val_loss: 2.0230 - val_accuracy: 0.4150\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 23s 492ms/step - loss: 1.9713 - accuracy: 0.4177 - val_loss: 1.8481 - val_accuracy: 0.3737\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 21s 450ms/step - loss: 1.7820 - accuracy: 0.4254 - val_loss: 1.6947 - val_accuracy: 0.4225\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 20s 417ms/step - loss: 1.6131 - accuracy: 0.4313 - val_loss: 1.5633 - val_accuracy: 0.4227\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 20s 426ms/step - loss: 1.4989 - accuracy: 0.4377 - val_loss: 1.5308 - val_accuracy: 0.4223\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 20s 427ms/step - loss: 1.4402 - accuracy: 0.4402 - val_loss: 1.5165 - val_accuracy: 0.4220\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 20s 428ms/step - loss: 1.4110 - accuracy: 0.4389 - val_loss: 1.4747 - val_accuracy: 0.3731\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 23s 488ms/step - loss: 1.3952 - accuracy: 0.4403 - val_loss: 1.4719 - val_accuracy: 0.4208\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 21s 442ms/step - loss: 1.3832 - accuracy: 0.4403 - val_loss: 1.5611 - val_accuracy: 0.4187\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 20s 420ms/step - loss: 1.3773 - accuracy: 0.4407 - val_loss: 1.5494 - val_accuracy: 0.4194\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 20s 424ms/step - loss: 1.3700 - accuracy: 0.4459 - val_loss: 1.4938 - val_accuracy: 0.3704\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 20s 430ms/step - loss: 1.3617 - accuracy: 0.4474 - val_loss: 1.5511 - val_accuracy: 0.4166\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 22s 464ms/step - loss: 1.3629 - accuracy: 0.4472 - val_loss: 1.5625 - val_accuracy: 0.4163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2b402aa5650>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_nb_words, 100, input_length=MAX_SEQUENCE_LENGTH))\n",
    "artist_input = Input(shape=(1,), name='artist_input')\n",
    "artist_repeated = RepeatVector(300)(artist_input)\n",
    "artist_reshaped = Reshape((300, 1))(artist_repeated)\n",
    "concatenated = Concatenate(axis=-1)([model.output, artist_reshaped])\n",
    "\n",
    "model.add(SimpleRNN(10))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model = Model(inputs=[model.input, artist_input], outputs=model.output)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit([train_data2, train_artist_encoded], labels_train2,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=([val_data2, val_artist_encoded], labels_val2),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623294a",
   "metadata": {},
   "source": [
    "#### Single Layer LSTM Model:\n",
    "Similarly for the signle layer LSTM model:\n",
    "1) We use on the fly embeddings of size 100 for our tokenized text\n",
    "2) Then we define the input layer for the Artist information and repeat it 300 times to match the lyric embeddings length. Then we reshape the artist input and finally concatenate it with the on the fly embeddings for the lyrics\n",
    "2) For the LSTM layer, we set the state size and return_sequences to True to output all the hidden states for each timestep of the text sequence. The we flatten the output to pass to the Batch Normalization layer.\n",
    "2) We use Batch Normalization and Dropout of 20% to prevent overfitting as Batch Normalization helps to stabilize and speed up the training process by normalizing the activations and Dropout encourages a sparser network reliant on more independant neurons. \n",
    "3) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "4) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "5) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "6) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "7) We fit the model on the training data(the tokenized lyrics and the encoded artist), providing the batch size, number of epochs, the validation data(the tokenized lyrics and the encoded artist) and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f78cd9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " embedding_1_input (InputLa  [(None, 300)]                0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 300, 100)             2000000   ['embedding_1_input[0][0]']   \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 300, 10)              4440      ['embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 3000)                 0         ['lstm[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 3000)                 12000     ['flatten[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 3000)                 0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " artist_input (InputLayer)   [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 10)                   30010     ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2046450 (7.81 MB)\n",
      "Trainable params: 2040450 (7.78 MB)\n",
      "Non-trainable params: 6000 (23.44 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "47/47 [==============================] - 56s 1s/step - loss: 1.7907 - accuracy: 0.3431 - val_loss: 1.4406 - val_accuracy: 0.4214\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 47s 1s/step - loss: 1.3082 - accuracy: 0.5224 - val_loss: 1.4251 - val_accuracy: 0.4232\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 47s 1s/step - loss: 1.1479 - accuracy: 0.5977 - val_loss: 1.4281 - val_accuracy: 0.4208\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 44s 933ms/step - loss: 0.9718 - accuracy: 0.6687 - val_loss: 1.4455 - val_accuracy: 0.4199\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 47s 1s/step - loss: 0.8197 - accuracy: 0.7258 - val_loss: 1.4819 - val_accuracy: 0.4118\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 41s 883ms/step - loss: 0.7103 - accuracy: 0.7670 - val_loss: 1.5365 - val_accuracy: 0.4138\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 42s 891ms/step - loss: 0.6300 - accuracy: 0.7960 - val_loss: 1.6264 - val_accuracy: 0.4125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2b4080541d0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_nb_words, 100, input_length=MAX_SEQUENCE_LENGTH))\n",
    "artist_input = Input(shape=(1,), name='artist_input')\n",
    "artist_repeated = RepeatVector(300)(artist_input)\n",
    "artist_reshaped = Reshape((300, 1))(artist_repeated)\n",
    "concatenated = Concatenate(axis=-1)([model.output, artist_reshaped])\n",
    "\n",
    "model.add(LSTM(state_size, return_sequences=True)) \n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model = Model(inputs=[model.input, artist_input], outputs=model.output)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit([train_data2, train_artist_encoded], labels_train2,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=([val_data2, val_artist_encoded], labels_val2),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e064d",
   "metadata": {},
   "source": [
    "#### Multi-Layer LSTM Model:\n",
    "Similarly for the multi layer LSTM model:\n",
    "\n",
    "1) We use on the fly embeddings of size 100 for our tokenized text\n",
    "2) Then we define the input layer for the Artist information and repeat it 300 times to match the lyric embeddings length. Then we reshape the artist input and finally concatenate it with the on the fly embeddings for the lyrics\n",
    "2) For the LSTM layer, we set the state size to 10 and return_sequences to True to output all the hidden states for each timestep of the text sequence. The we flatten the output to pass to the Batch Normalization layer.\n",
    "3) We use Batch Normalization and Dropout of 20% to prevent overfitting as Batch Normalization helps to stabilize and speed up the training process by normalizing the activations and Dropout encourages a sparser network reliant on more independant neurons.\n",
    "4) Then we add the 2nd LSTM layer with a state size of 10.\n",
    "5) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "6) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "7) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "8) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "9) We fit the model on the training data, providing the batch size, number of epochs, the validation data and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcd6b03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_nb_words, 100))\n",
    "artist_input = Input(shape=(1,), name='artist_input')\n",
    "artist_repeated = RepeatVector(300)(artist_input)\n",
    "artist_reshaped = Reshape((300, 1))(artist_repeated)\n",
    "concatenated = Concatenate(axis=-1)([model.output, artist_reshaped])\n",
    "\n",
    "model.add(LSTM(10, return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(num_classes, activation=tf.nn.softmax))\n",
    "\n",
    "model = Model(inputs=[model.input, artist_input], outputs=model.output)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit([train_data2, train_artist_encoded], labels_train2,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=([val_data2, val_artist_encoded], labels_val2),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d608d131",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "#### On the fly embeddings\n",
    "For the on the fly embeddings model:\n",
    "1) We use on the fly embeddings of size 100 for our tokenized text\n",
    "2) Then we define the input layer for the Artist information and repeat it 300 times to match the lyric embeddings length. Then we reshape the artist input. \n",
    "2) We use GlobalAveragePooling1D to reduce the dimensionality of the embeddings to feed the output to the following Dense layers.\n",
    "2) Then we concatenate the lyrics embeddings and the artist input\n",
    "3) Then we use a dense layer of 64 connected neurons with the relu activation function\n",
    "3) We use Dropout of 20% to prevent overfitting as it encourages a sparser network reliant on more independant neurons.\n",
    "3) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "4) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "5) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "6) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "7) We fit the model on the training data, providing the batch size, number of epochs, the validation data and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3d5b99eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jess6\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_24\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_24\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ lyrics_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> │ lyrics_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ artist_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ artist_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,528</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │ dense_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ lyrics_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │  \u001b[38;5;34m2,000,000\u001b[0m │ lyrics_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ artist_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m101\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ artist_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m6,528\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │        \u001b[38;5;34m650\u001b[0m │ dense_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,007,178</span> (7.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,007,178\u001b[0m (7.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,007,178</span> (7.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,007,178\u001b[0m (7.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 29ms/step - accuracy: 0.3495 - loss: 19.2723 - val_accuracy: 0.3502 - val_loss: 1.8395\n",
      "Epoch 2/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 34ms/step - accuracy: 0.3930 - loss: 2.0905 - val_accuracy: 0.4178 - val_loss: 2.2095\n",
      "Epoch 3/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 38ms/step - accuracy: 0.3965 - loss: 2.0586 - val_accuracy: 0.4170 - val_loss: 2.0609\n",
      "Epoch 4/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 35ms/step - accuracy: 0.4123 - loss: 1.9551 - val_accuracy: 0.4168 - val_loss: 1.6852\n",
      "Epoch 5/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 35ms/step - accuracy: 0.4299 - loss: 1.9100 - val_accuracy: 0.3951 - val_loss: 1.8047\n",
      "Epoch 6/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 36ms/step - accuracy: 0.4546 - loss: 1.7957 - val_accuracy: 0.3803 - val_loss: 2.3049\n",
      "Epoch 7/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 24ms/step - accuracy: 0.4627 - loss: 1.7408 - val_accuracy: 0.3948 - val_loss: 1.9427\n",
      "Epoch 8/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 32ms/step - accuracy: 0.4801 - loss: 1.6683 - val_accuracy: 0.4092 - val_loss: 2.1516\n",
      "Epoch 9/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 37ms/step - accuracy: 0.5038 - loss: 1.5493 - val_accuracy: 0.3583 - val_loss: 2.0047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b7c714f8d0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_input = Input(shape=(300,), name='lyrics_input')\n",
    "artist_input = Input(shape=(train_artist_encoded.shape[1],), name='artist_input')\n",
    "\n",
    "embedding_layer = Embedding(max_nb_words, 100, input_length=300)(lyrics_input)\n",
    "global_avg_pooling_lyrics = GlobalAveragePooling1D()(embedding_layer)\n",
    "\n",
    "concatenated = Concatenate(axis=-1)([global_avg_pooling_lyrics, artist_input])\n",
    "\n",
    "dense1 = Dense(64, activation='relu')(concatenated)\n",
    "dropout1 = Dropout(0.2)(dense1) \n",
    "output_layer = Dense(num_classes, activation='softmax')(dropout1)\n",
    "\n",
    "model = Model(inputs=[lyrics_input, artist_input], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit([train_data2, train_artist_encoded], labels_train2,\n",
    "          epochs=20,\n",
    "          validation_data=([val_data2, val_artist_encoded], labels_val2),\n",
    "          callbacks=[early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c396465",
   "metadata": {},
   "source": [
    "#### Pretrained embeddings\n",
    "\n",
    "As previously done, we load the pre trained embeddings model from Tensorflow hub and use it to create our embeddings for the Lyrics and the Artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "71d29e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "embeding_model = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
    "embeddings_train2 = embeding_model(train2.Lyrics)\n",
    "embeddings_val2 = embeding_model(val2.Lyrics)\n",
    "embeddings_artist_train = embeding_model(train2.Artist)\n",
    "embeddings_artist_val = embeding_model(val2.Artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaf61a2",
   "metadata": {},
   "source": [
    "For the pre trained embeddings model:\n",
    "1) We concatenate the artist and the lyrics embeddings gotten from the pre trained model.\n",
    "2) Then we use a dense layer of 64 connected neurons with the relu activation function\n",
    "3) We use Dropout of 20% to prevent overfitting as it encourages a sparser network reliant on more independant neurons.\n",
    "3) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "4) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "5) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "6) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "7) We fit the model on the training data that has been embedded using the pre trained embeddings, providing the batch size, number of epochs, the validation data that has also been embedded using the pre trained embeddings and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f33f5c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ lyrics_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ artist_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lyrics_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ artist_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,464</span> │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ lyrics_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ artist_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ lyrics_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ artist_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m6,464\u001b[0m │ concatenate_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │        \u001b[38;5;34m650\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,114</span> (27.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,114\u001b[0m (27.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,114</span> (27.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,114\u001b[0m (27.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.4679 - loss: 1.4960 - val_accuracy: 0.5660 - val_loss: 1.1999\n",
      "Epoch 2/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5603 - loss: 1.2353 - val_accuracy: 0.5900 - val_loss: 1.1537\n",
      "Epoch 3/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.5810 - loss: 1.1820 - val_accuracy: 0.5928 - val_loss: 1.1277\n",
      "Epoch 4/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.5920 - loss: 1.1555 - val_accuracy: 0.6140 - val_loss: 1.1018\n",
      "Epoch 5/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.5995 - loss: 1.1322 - val_accuracy: 0.6200 - val_loss: 1.0815\n",
      "Epoch 6/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.6056 - loss: 1.1202 - val_accuracy: 0.6338 - val_loss: 1.0584\n",
      "Epoch 7/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.6176 - loss: 1.0929 - val_accuracy: 0.6431 - val_loss: 1.0422\n",
      "Epoch 8/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.6183 - loss: 1.0946 - val_accuracy: 0.6471 - val_loss: 1.0280\n",
      "Epoch 9/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.6296 - loss: 1.0583 - val_accuracy: 0.6568 - val_loss: 1.0146\n",
      "Epoch 10/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.6325 - loss: 1.0571 - val_accuracy: 0.6574 - val_loss: 1.0035\n",
      "Epoch 11/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.6369 - loss: 1.0346 - val_accuracy: 0.6613 - val_loss: 0.9928\n",
      "Epoch 12/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.6396 - loss: 1.0372 - val_accuracy: 0.6682 - val_loss: 0.9732\n",
      "Epoch 13/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.6478 - loss: 1.0332 - val_accuracy: 0.6743 - val_loss: 0.9642\n",
      "Epoch 14/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.6486 - loss: 1.0140 - val_accuracy: 0.6778 - val_loss: 0.9571\n",
      "Epoch 15/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.6571 - loss: 1.0022 - val_accuracy: 0.6837 - val_loss: 0.9471\n",
      "Epoch 16/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.6561 - loss: 1.0025 - val_accuracy: 0.6906 - val_loss: 0.9285\n",
      "Epoch 17/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.6573 - loss: 0.9953 - val_accuracy: 0.6978 - val_loss: 0.9253\n",
      "Epoch 18/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.6635 - loss: 0.9826 - val_accuracy: 0.7020 - val_loss: 0.9170\n",
      "Epoch 19/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.6676 - loss: 0.9721 - val_accuracy: 0.7047 - val_loss: 0.9143\n",
      "Epoch 20/20\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.6684 - loss: 0.9791 - val_accuracy: 0.7077 - val_loss: 0.9064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1679eedbfd0>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_input = Input(shape=(50,), name='lyrics_input')\n",
    "artist_input = Input(shape=(50,), name='artist_input')\n",
    "\n",
    "concatenated = Concatenate(axis=-1)([lyrics_input, artist_input])\n",
    "dense1 = Dense(64, activation='relu')(concatenated)\n",
    "dropout1 = Dropout(0.5)(dense1)  \n",
    "output_layer = Dense(num_classes, activation='softmax')(dropout1)\n",
    "\n",
    "model = Model(inputs=[lyrics_input, artist_input], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit([embeddings_train2, embeddings_artist_train], labels_train2, \n",
    "          epochs=20, \n",
    "          validation_data=([embeddings_val2, embeddings_artist_val], labels_val2), \n",
    "          callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db16d1ba",
   "metadata": {},
   "source": [
    "#### Traditional text approach\n",
    "\n",
    "We use TF-IDF to transform the text into a numerical representation. Then we feed that embedding to a Logistic Regression Model used for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ef7bf1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)#we define the TF-IDFVectorizer using a maximum features of 5000\n",
    "\n",
    "combined_lyrics_artist_train = train2.Lyrics + ' ' + train2.Artist#we combine the lyrics and the artist information into a single vector\n",
    "combined_lyrics_artist_val = val2.Lyrics + ' ' + val2.Artist\n",
    "\n",
    "tfidf_train2 = tfidf_vectorizer.fit_transform(combined_lyrics_artist_train)#we fit and transform the combined lyrics and artist training and validation sets\n",
    "tfidf_val2 = tfidf_vectorizer.transform(combined_lyrics_artist_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3e7e9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Accuracy: 0.622781320006893\n"
     ]
    }
   ],
   "source": [
    "clf_tfidf = LogisticRegression(max_iter=1000)#we use the Logistic Regression Model and set the max-iter parameter to speed up training\n",
    "clf_tfidf.fit(tfidf_train2, train_labels2)\n",
    "preds_tfidf2 = clf_tfidf.predict(tfidf_val2)#we make predictions on our validation set\n",
    "accuracy_tfidf = accuracy_score(val_labels2, preds_tfidf2)#we calculate the accuracy of those predictions\n",
    "print(f\"TF-IDF Accuracy: {accuracy_tfidf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8492bb0",
   "metadata": {},
   "source": [
    "\n",
    "### CNN for Text Classification\n",
    "#### CNNs with same kernel sizes:\n",
    "\n",
    "For the CNN with same kernel size model:\n",
    "1) We use on the fly embeddings of size 100 for our tokenized text\n",
    "2) Then we define the input layer for the Artist information and repeat it 300 times to match the lyric embeddings length. Then we reshape the artist input and finally concatenate it with the on the fly embeddings for the lyrics\n",
    "2) Then we use a 1 Dimension Convolutional layer, with 25 different filters, a kernel size of 3, no additional padding and a stride of 1 to capture more local features\n",
    "3) Then we use Batch Normalization to prevent overfitting as it helps to stabilize and speed up the training process by normalizing the data.\n",
    "4) We use the relu activation function, followed by Max Pooling to reduce the size of our feature maps\n",
    "5) We use Dropout of 20% to prevent overfitting as it encourages a sparser network reliant on more independant neurons. \n",
    "6) We repeat this structure to have 3 blocks of convolutional features.\n",
    "7) Then we flatten the output and feed it to a dense layer of 50 connected neurons using the relu activation function\n",
    "8) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "9) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "10) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "11) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "12) We fit the model on the training data, providing the batch size, number of epochs, the validation data and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2a8214be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jess6\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " embedding_3_input (InputLa  [(None, 300)]                0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)     (None, 300, 100)             2000000   ['embedding_3_input[0][0]']   \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 298, 25)              7525      ['embedding_3[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 298, 25)              100       ['conv1d[0][0]']              \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 298, 25)              0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1  (None, 149, 25)              0         ['activation[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 149, 25)              0         ['max_pooling1d[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 147, 25)              1900      ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 147, 25)              100       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 147, 25)              0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPoolin  (None, 73, 25)               0         ['activation_1[0][0]']        \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 73, 25)               0         ['max_pooling1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 71, 30)               2280      ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 71, 30)               120       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 71, 30)               0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPoolin  (None, 35, 30)               0         ['activation_2[0][0]']        \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 35, 30)               0         ['max_pooling1d_2[0][0]']     \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 1050)                 0         ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 50)                   52550     ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " artist_input (InputLayer)   [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 10)                   510       ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2065085 (7.88 MB)\n",
      "Trainable params: 2064925 (7.88 MB)\n",
      "Non-trainable params: 160 (640.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "47/47 [==============================] - 54s 1s/step - loss: 1.5386 - accuracy: 0.3969 - val_loss: 1.9150 - val_accuracy: 0.3789\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 43s 919ms/step - loss: 1.4342 - accuracy: 0.4179 - val_loss: 1.6935 - val_accuracy: 0.4168\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 41s 863ms/step - loss: 1.4193 - accuracy: 0.4258 - val_loss: 1.5875 - val_accuracy: 0.4019\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 43s 911ms/step - loss: 1.4001 - accuracy: 0.4420 - val_loss: 1.5418 - val_accuracy: 0.3708\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 43s 925ms/step - loss: 1.3673 - accuracy: 0.4634 - val_loss: 1.5678 - val_accuracy: 0.3704\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 42s 893ms/step - loss: 1.3191 - accuracy: 0.4894 - val_loss: 1.5432 - val_accuracy: 0.3703\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 41s 877ms/step - loss: 1.2582 - accuracy: 0.5155 - val_loss: 1.6171 - val_accuracy: 0.3702\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 41s 869ms/step - loss: 1.1771 - accuracy: 0.5540 - val_loss: 1.6401 - val_accuracy: 0.3761\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 41s 880ms/step - loss: 1.0937 - accuracy: 0.5907 - val_loss: 1.8266 - val_accuracy: 0.3770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2b40b3fa0d0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_nb_words, 100, input_length=MAX_SEQUENCE_LENGTH))\n",
    "artist_input = Input(shape=(1,), name='artist_input')\n",
    "artist_repeated = RepeatVector(300)(artist_input)\n",
    "artist_reshaped = Reshape((300, 1))(artist_repeated)\n",
    "concatenated = Concatenate(axis=-1)([model.output, artist_reshaped])\n",
    "\n",
    "model.add(Conv1D(filters=25, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(filters=25, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(filters=30, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(num_classes, activation=tf.nn.softmax))\n",
    "\n",
    "model = Model(inputs=[model.input, artist_input], outputs=model.output)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit([train_data2, train_artist_encoded], labels_train2,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=([val_data2, val_artist_encoded], labels_val2),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8c737",
   "metadata": {},
   "source": [
    "#### CNNs with different kernel sizes:\n",
    "\n",
    "For the CNN with different kernel size model, we vary the kernel size for each of the convolutional layers, starting from 3 to 5, to capture more general features in the further layers:\n",
    "1) We use on the fly embeddings of size 100 for our tokenized text\n",
    "2) Then we define the input layer for the Artist information and repeat it 300 times to match the lyric embeddings length. Then we reshape the artist input and finally concatenate it with the on the fly embeddings for the lyrics\n",
    "2) Then we use a 1 Dimension Convolutional layer, with 25 different filters, a kernel size of 3, no additional padding and a stride of 1 to capture more local features\n",
    "3) Then we use Batch Normalization to prevent overfitting as it helps to stabilize and speed up the training process by normalizing the data.\n",
    "4) We use the relu activation function, followed by Max Pooling to reduce the size of our feature maps\n",
    "5) We use Dropout of 20% to prevent overfitting as it encourages a sparser network reliant on more independant neurons. \n",
    "6) We repeat this structure to have 3 blocks of convolutional features, varying the kernel sizes going from 3 to 5, to capture more general features in the further layers\n",
    "7) Then we flatten the output and feed it to a dense layer of 50 connected neurons using the relu activation function\n",
    "8) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "9) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "10) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "11) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "12) We fit the model on the training data, providing the batch size, number of epochs, the validation data and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d22b6fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " embedding_5_input (InputLa  [(None, 300)]                0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)     (None, 300, 100)             2000000   ['embedding_5_input[0][0]']   \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 298, 25)              7525      ['embedding_5[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 298, 25)              100       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 298, 25)              0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPoolin  (None, 149, 25)              0         ['activation_6[0][0]']        \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 149, 25)              0         ['max_pooling1d_6[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 146, 25)              2525      ['dropout_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 146, 25)              100       ['conv1d_7[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 146, 25)              0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max_pooling1d_7 (MaxPoolin  (None, 73, 25)               0         ['activation_7[0][0]']        \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 73, 25)               0         ['max_pooling1d_7[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 69, 30)               3780      ['dropout_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 69, 30)               120       ['conv1d_8[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 69, 30)               0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max_pooling1d_8 (MaxPoolin  (None, 34, 30)               0         ['activation_8[0][0]']        \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 34, 30)               0         ['max_pooling1d_8[0][0]']     \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)         (None, 1020)                 0         ['dropout_11[0][0]']          \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 50)                   51050     ['flatten_3[0][0]']           \n",
      "                                                                                                  \n",
      " artist_input (InputLayer)   [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 10)                   510       ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2065710 (7.88 MB)\n",
      "Trainable params: 2065550 (7.88 MB)\n",
      "Non-trainable params: 160 (640.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "47/47 [==============================] - 47s 912ms/step - loss: 1.5203 - accuracy: 0.3946 - val_loss: 1.8000 - val_accuracy: 0.4212\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 41s 875ms/step - loss: 1.4271 - accuracy: 0.4185 - val_loss: 1.5902 - val_accuracy: 0.4212\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 42s 892ms/step - loss: 1.4182 - accuracy: 0.4196 - val_loss: 1.5118 - val_accuracy: 0.4212\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 43s 923ms/step - loss: 1.4069 - accuracy: 0.4180 - val_loss: 1.4717 - val_accuracy: 0.4212\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 43s 921ms/step - loss: 1.3834 - accuracy: 0.4212 - val_loss: 1.4891 - val_accuracy: 0.4212\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 52s 1s/step - loss: 1.3433 - accuracy: 0.4300 - val_loss: 1.4631 - val_accuracy: 0.4212\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 42s 898ms/step - loss: 1.2921 - accuracy: 0.4407 - val_loss: 1.4881 - val_accuracy: 0.3750\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 43s 924ms/step - loss: 1.2287 - accuracy: 0.4616 - val_loss: 1.5154 - val_accuracy: 0.3934\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 45s 952ms/step - loss: 1.1567 - accuracy: 0.5004 - val_loss: 1.5872 - val_accuracy: 0.3945\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 41s 880ms/step - loss: 1.0815 - accuracy: 0.5491 - val_loss: 1.7017 - val_accuracy: 0.3261\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 43s 920ms/step - loss: 0.9976 - accuracy: 0.5957 - val_loss: 2.0739 - val_accuracy: 0.1972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2b458c9c0d0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_nb_words, 100, input_length=MAX_SEQUENCE_LENGTH))\n",
    "artist_input = Input(shape=(1,), name='artist_input')\n",
    "artist_repeated = RepeatVector(300)(artist_input)\n",
    "artist_reshaped = Reshape((300, 1))(artist_repeated)\n",
    "concatenated = Concatenate(axis=-1)([model.output, artist_reshaped])\n",
    "\n",
    "model.add(Conv1D(filters=25, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Dropout(0.2)) \n",
    "\n",
    "model.add(Conv1D(filters=25, kernel_size=4, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Dropout(0.2)) \n",
    "\n",
    "model.add(Conv1D(filters=30, kernel_size=5, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Dropout(0.2)) \n",
    "\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(num_classes, activation=tf.nn.softmax))\n",
    "\n",
    "model = Model(inputs=[model.input, artist_input], outputs=model.output)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit([train_data2, train_artist_encoded], labels_train2,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=([val_data2, val_artist_encoded], labels_val2),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897b5cdf",
   "metadata": {},
   "source": [
    "#### CNN as an additional layer before a LSTM solution:\n",
    "For the CNN was an additional layer before a LSTM solution, we keep the same kernel size as that performs better than the differnet kernel sizes. We try 2 different versions, the first being:\n",
    "1) We use on the fly embeddings of size 100 for our tokenized text\n",
    "2) Then we define the input layer for the Artist information and repeat it 300 times to match the lyric embeddings length. Then we reshape the artist input and finally concatenate it with the on the fly embeddings for the lyrics\n",
    "2) Then we use a 1 Dimension Convolutional layer, with 25 different filters, a kernel size of 3, no additional padding and a stride of 1 to capture more local features\n",
    "3) Then we use Batch Normalization to prevent overfitting as it helps to stabilize and speed up the training process by normalizing the data.\n",
    "4) We use the relu activation function, followed by Max Pooling to reduce the size of our feature maps\n",
    "5) We use Dropout of 20% to prevent overfitting as it encourages a sparser network reliant on more independant neurons. \n",
    "6) We repeat this structure to have 3 blocks of convolutional features\n",
    "7) Then we add the LSTM model with a state size of 10, followed by Batch Normalization and Dropout to prevent overfitting\n",
    "8) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "9) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "10) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "11) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "12) We fit the model on the training data, providing the batch size, number of epochs, the validation data and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e8d290e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " embedding_6_input (InputLa  [(None, 300)]                0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)     (None, 300, 100)             2000000   ['embedding_6_input[0][0]']   \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 298, 25)              7525      ['embedding_6[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 298, 25)              100       ['conv1d_9[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_9 (Activation)   (None, 298, 25)              0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPoolin  (None, 149, 25)              0         ['activation_9[0][0]']        \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)        (None, 149, 25)              0         ['max_pooling1d_9[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 147, 25)              1900      ['dropout_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 147, 25)              100       ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_10 (Activation)  (None, 147, 25)              0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max_pooling1d_10 (MaxPooli  (None, 73, 25)               0         ['activation_10[0][0]']       \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)        (None, 73, 25)               0         ['max_pooling1d_10[0][0]']    \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)          (None, 71, 30)               2280      ['dropout_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 71, 30)               120       ['conv1d_11[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_11 (Activation)  (None, 71, 30)               0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max_pooling1d_11 (MaxPooli  (None, 35, 30)               0         ['activation_11[0][0]']       \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 35, 30)               0         ['max_pooling1d_11[0][0]']    \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               (None, 10)                   1640      ['dropout_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 10)                   40        ['lstm_3[0][0]']              \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 10)                   0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " artist_input (InputLayer)   [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 10)                   110       ['dropout_15[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2013815 (7.68 MB)\n",
      "Trainable params: 2013635 (7.68 MB)\n",
      "Non-trainable params: 180 (720.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "47/47 [==============================] - 49s 921ms/step - loss: 2.5149 - accuracy: 0.1766 - val_loss: 2.1786 - val_accuracy: 0.3710\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 49s 1s/step - loss: 2.2966 - accuracy: 0.2565 - val_loss: 2.1048 - val_accuracy: 0.3704\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 45s 951ms/step - loss: 2.1589 - accuracy: 0.3318 - val_loss: 1.9941 - val_accuracy: 0.3698\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 43s 911ms/step - loss: 2.0074 - accuracy: 0.3658 - val_loss: 1.8317 - val_accuracy: 0.3725\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 50s 1s/step - loss: 1.8490 - accuracy: 0.3774 - val_loss: 1.6849 - val_accuracy: 0.3723\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 52s 1s/step - loss: 1.6916 - accuracy: 0.3961 - val_loss: 1.5631 - val_accuracy: 0.4212\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 48s 1s/step - loss: 1.5759 - accuracy: 0.4181 - val_loss: 1.4945 - val_accuracy: 0.4212\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 49s 1s/step - loss: 1.4986 - accuracy: 0.4429 - val_loss: 1.4715 - val_accuracy: 0.4212\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 44s 944ms/step - loss: 1.4474 - accuracy: 0.4675 - val_loss: 1.4573 - val_accuracy: 0.4205\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 40s 860ms/step - loss: 1.4001 - accuracy: 0.4940 - val_loss: 1.4587 - val_accuracy: 0.4168\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 43s 908ms/step - loss: 1.3562 - accuracy: 0.5280 - val_loss: 1.5144 - val_accuracy: 0.3801\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 42s 905ms/step - loss: 1.3074 - accuracy: 0.5623 - val_loss: 1.6211 - val_accuracy: 0.3740\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 42s 890ms/step - loss: 1.2641 - accuracy: 0.5910 - val_loss: 1.6054 - val_accuracy: 0.3907\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 42s 895ms/step - loss: 1.2192 - accuracy: 0.6192 - val_loss: 1.6520 - val_accuracy: 0.3970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2b4592b41d0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1st version\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_nb_words, 100, input_length=MAX_SEQUENCE_LENGTH))\n",
    "artist_input = Input(shape=(1,), name='artist_input')\n",
    "artist_repeated = RepeatVector(300)(artist_input)\n",
    "artist_reshaped = Reshape((300, 1))(artist_repeated)\n",
    "concatenated = Concatenate(axis=-1)([model.output, artist_reshaped])\n",
    "\n",
    "model.add(Conv1D(filters=25, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(filters=25, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Dropout(0.2)) \n",
    "\n",
    "model.add(Conv1D(filters=30, kernel_size=3, padding='valid', strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Dropout(0.2))  \n",
    "\n",
    "model.add(LSTM(10))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))  \n",
    "model.add(Dense(num_classes, activation=tf.nn.softmax))\n",
    "\n",
    "model = Model(inputs=[model.input, artist_input], outputs=model.output)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit([train_data2, train_artist_encoded], labels_train2,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=([val_data2, val_artist_encoded], labels_val2),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77926a1",
   "metadata": {},
   "source": [
    "For the second version:\n",
    "1) We use on the fly embeddings of size 100 for our tokenized text\n",
    "2) Then we use a 1 Dimension Convolutional layer, with 25 different filters, a kernel size of 3, no additional padding and a stride of 1 to capture more local features\n",
    "3) Then we use Batch Normalization to prevent overfitting as it helps to stabilize and speed up the training process by normalizing the data.\n",
    "4) We use the relu activation function, followed by Max Pooling to reduce the size of our feature maps\n",
    "5) We use Dropout of 20% to prevent overfitting as it encourages a sparser network reliant on more independant neurons. \n",
    "6) We repeat this structure to have 3 blocks of convolutional features\n",
    "7) Then we flatten the output, concatente it with the artist input and reshape it to pass it to the LSTM model\n",
    "7) We define LSTM model with a state size of 10, followed by Batch Normalization to prevent overfitting\n",
    "8) The final layer is a softmax layer as we are in a multiclass classification problem and we want to predcit only one class\n",
    "9) Similarly we use the categorical cross entropy loss function for the same reason as we have a multiclass classification problem\n",
    "10) We use the Adam optimizer as it is the best performing and the models are evaluated on the accuracy metric\n",
    "11) We use the Early Stopping callback to prevent overfitting and speed up the training process, as this monitors the validation loss and stops when there are no signinficant improvements.\n",
    "12) We fit the model on the training data, providing the batch size, number of epochs, the validation data and the early stopping callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aa468665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " lyrics_input (InputLayer)   [(None, 300)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_8 (Embedding)     (None, 300, 100)             2000000   ['lyrics_input[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)          (None, 298, 25)              7525      ['embedding_8[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 298, 25)              100       ['conv1d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_15 (MaxPooli  (None, 149, 25)              0         ['batch_normalization_19[0][0]\n",
      " ng1D)                                                              ']                            \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)          (None, 147, 25)              1900      ['max_pooling1d_15[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 147, 25)              100       ['conv1d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_16 (MaxPooli  (None, 73, 25)               0         ['batch_normalization_20[0][0]\n",
      " ng1D)                                                              ']                            \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)          (None, 71, 30)               2280      ['max_pooling1d_16[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, 71, 30)               120       ['conv1d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_17 (MaxPooli  (None, 35, 30)               0         ['batch_normalization_21[0][0]\n",
      " ng1D)                                                              ']                            \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)         (None, 1050)                 0         ['max_pooling1d_17[0][0]']    \n",
      "                                                                                                  \n",
      " artist_input (InputLayer)   [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate  (None, 1051)                 0         ['flatten_5[0][0]',           \n",
      " )                                                                   'artist_input[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_7 (Reshape)         (None, 1051, 1)              0         ['concatenate_7[0][0]']       \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)               (None, 10)                   480       ['reshape_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_22 (Ba  (None, 10)                   40        ['lstm_4[0][0]']              \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 10)                   110       ['batch_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2012655 (7.68 MB)\n",
      "Trainable params: 2012475 (7.68 MB)\n",
      "Non-trainable params: 180 (720.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "47/47 [==============================] - 191s 4s/step - loss: 2.3058 - accuracy: 0.2722 - val_loss: 2.1940 - val_accuracy: 0.0551\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 184s 4s/step - loss: 2.1471 - accuracy: 0.4021 - val_loss: 2.0554 - val_accuracy: 0.3765\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 177s 4s/step - loss: 1.9691 - accuracy: 0.4213 - val_loss: 1.8691 - val_accuracy: 0.3757\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 208s 4s/step - loss: 1.7561 - accuracy: 0.4101 - val_loss: 1.6990 - val_accuracy: 0.3771\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 205s 4s/step - loss: 1.5741 - accuracy: 0.4140 - val_loss: 1.5863 - val_accuracy: 0.4212\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 232s 5s/step - loss: 1.4574 - accuracy: 0.4354 - val_loss: 1.5596 - val_accuracy: 0.4206\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 244s 5s/step - loss: 1.3976 - accuracy: 0.4552 - val_loss: 1.6200 - val_accuracy: 0.4203\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 196s 4s/step - loss: 1.3675 - accuracy: 0.4603 - val_loss: 1.7013 - val_accuracy: 0.4119\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 201s 4s/step - loss: 1.3515 - accuracy: 0.4607 - val_loss: 1.7897 - val_accuracy: 0.4106\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 195s 4s/step - loss: 1.3383 - accuracy: 0.4589 - val_loss: 1.9315 - val_accuracy: 0.4097\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 219s 5s/step - loss: 1.3336 - accuracy: 0.4630 - val_loss: 1.9929 - val_accuracy: 0.4199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2b459c89810>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2nd version\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "lyrics_input = Input(shape=(300,), name='lyrics_input')\n",
    "lyrics_embedding = Embedding(max_nb_words, 100, input_length=300)(lyrics_input)\n",
    "\n",
    "conv1d_1 = Conv1D(filters=25, kernel_size=3, padding='valid', activation='relu')(lyrics_embedding)\n",
    "batchnorm_1 = BatchNormalization()(conv1d_1)\n",
    "maxpooling_1 = MaxPooling1D(pool_size=2)(batchnorm_1)\n",
    "\n",
    "conv1d_2 = Conv1D(filters=25, kernel_size=3, padding='valid', activation='relu')(maxpooling_1)\n",
    "batchnorm_2 = BatchNormalization()(conv1d_2)\n",
    "maxpooling_2 = MaxPooling1D(pool_size=2)(batchnorm_2)\n",
    "\n",
    "conv1d_3 = Conv1D(filters=30, kernel_size=3, padding='valid', activation='relu')(maxpooling_2)\n",
    "batchnorm_3 = BatchNormalization()(conv1d_3)\n",
    "maxpooling_3 = MaxPooling1D(pool_size=2)(batchnorm_3)\n",
    "\n",
    "flatten = Flatten()(maxpooling_3)\n",
    "artist_input = Input(shape=(1,), name='artist_input')\n",
    "concatenated = concatenate([flatten, artist_input])\n",
    "reshaped = Reshape((-1, 1))(concatenated)\n",
    "\n",
    "lstm = LSTM(10)(reshaped)\n",
    "batchnorm_lstm = BatchNormalization()(lstm)\n",
    "\n",
    "output = Dense(num_classes, activation='softmax')(batchnorm_lstm)\n",
    "\n",
    "model = Model(inputs=[lyrics_input, artist_input], outputs=output)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "model.fit([train_data2, train_artist_encoded], labels_train2,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=([val_data2, val_artist_encoded], labels_val2),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c7461c",
   "metadata": {},
   "source": [
    "#### Comparison to Non-Neural Methods\n",
    "\n",
    "We compare our models to a traditional ML model: Decision Tree Classifier.\n",
    "\n",
    "First we concatenate our tokenized lyrics and the encoded artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5d1f18e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_artist = np.concatenate((train_data2, train_artist_encoded), axis=1)\n",
    "val_data_with_artist = np.concatenate((val_data2, val_artist_encoded), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af287bfe",
   "metadata": {},
   "source": [
    "We do a grid search to find the best hyperparmeters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b9c60c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [4, 8], \n",
    "    'min_samples_split': [2, 5, 10],  \n",
    "    'min_samples_leaf': [1, 2, 4], \n",
    "}\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(train_data_with_artist, labels_train)\n",
    "\n",
    "predictions = grid_search.predict(val_data_with_artist)# we make the predictions on the validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b783c1f",
   "metadata": {},
   "source": [
    "We get the accuracy and the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "35ccf71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.29691538859210753\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(labels_val, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f0ae1730",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 8, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.35604486627563137\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3bea08",
   "metadata": {},
   "source": [
    "#### Preprocessing the test dataset to save in google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0a3a1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "947921c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song</th>\n",
       "      <th>Song year</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>craftsmanship</td>\n",
       "      <td>2005</td>\n",
       "      <td>buck-65</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>Most folks spend their days daydreaming of fin...</td>\n",
       "      <td>8294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>come-on-out</td>\n",
       "      <td>2012</td>\n",
       "      <td>the-elwins</td>\n",
       "      <td>Indie</td>\n",
       "      <td>Take your cold hands and put them on my face\\n...</td>\n",
       "      <td>21621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>riot</td>\n",
       "      <td>2013</td>\n",
       "      <td>bullet-for-my-valentine</td>\n",
       "      <td>Metal</td>\n",
       "      <td>Are you ready it's time for war\\nWe'll break d...</td>\n",
       "      <td>3301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that-s-what-girls-do</td>\n",
       "      <td>2007</td>\n",
       "      <td>dream-street</td>\n",
       "      <td>Pop</td>\n",
       "      <td>You ask me why I change the color of my hair\\n...</td>\n",
       "      <td>2773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>believe-in-a-dollar</td>\n",
       "      <td>2012</td>\n",
       "      <td>cassidy</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>Do you believe in magic in a young girl's hear...</td>\n",
       "      <td>16797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Song  Song year                   Artist    Genre  \\\n",
       "0         craftsmanship       2005                  buck-65  Hip-Hop   \n",
       "1           come-on-out       2012               the-elwins    Indie   \n",
       "2                  riot       2013  bullet-for-my-valentine    Metal   \n",
       "3  that-s-what-girls-do       2007             dream-street      Pop   \n",
       "4   believe-in-a-dollar       2012                  cassidy  Hip-Hop   \n",
       "\n",
       "                                              Lyrics  Track_id  \n",
       "0  Most folks spend their days daydreaming of fin...      8294  \n",
       "1  Take your cold hands and put them on my face\\n...     21621  \n",
       "2  Are you ready it's time for war\\nWe'll break d...      3301  \n",
       "3  You ask me why I change the color of my hair\\n...      2773  \n",
       "4  Do you believe in magic in a young girl's hear...     16797  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863bdc00",
   "metadata": {},
   "source": [
    "We only take the Genre and Lyrics as our best neural network model was with lyrics only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f61efac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>Most folks spend their days daydreaming of fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Indie</td>\n",
       "      <td>Take your cold hands and put them on my face\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Metal</td>\n",
       "      <td>Are you ready it's time for war\\nWe'll break d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pop</td>\n",
       "      <td>You ask me why I change the color of my hair\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>Do you believe in magic in a young girl's hear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7930</th>\n",
       "      <td>Rock</td>\n",
       "      <td>Tuesday night - 7:30\\nI hear a voice on the te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7931</th>\n",
       "      <td>Metal</td>\n",
       "      <td>Elite forces cloaked in fur un sensitive to pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7932</th>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>[Dr. Dre]\\nJourney with me\\nInto the mind of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7933</th>\n",
       "      <td>Rock</td>\n",
       "      <td>You can a look a hurricane right in the eye.\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7934</th>\n",
       "      <td>Rock</td>\n",
       "      <td>Deal not the truth till you find the proof\\nAn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7935 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Genre                                             Lyrics\n",
       "0     Hip-Hop  Most folks spend their days daydreaming of fin...\n",
       "1       Indie  Take your cold hands and put them on my face\\n...\n",
       "2       Metal  Are you ready it's time for war\\nWe'll break d...\n",
       "3         Pop  You ask me why I change the color of my hair\\n...\n",
       "4     Hip-Hop  Do you believe in magic in a young girl's hear...\n",
       "...       ...                                                ...\n",
       "7930     Rock  Tuesday night - 7:30\\nI hear a voice on the te...\n",
       "7931    Metal  Elite forces cloaked in fur un sensitive to pa...\n",
       "7932  Hip-Hop  [Dr. Dre]\\nJourney with me\\nInto the mind of a...\n",
       "7933     Rock  You can a look a hurricane right in the eye.\\n...\n",
       "7934     Rock  Deal not the truth till you find the proof\\nAn...\n",
       "\n",
       "[7935 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df_test = df_test.drop([\"Song\",\"Song year\", \"Artist\", \"Track_id\"], axis= 1)\n",
    "sub_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9253b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e12ced53",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=stopwords.words(\"english\")\n",
    "sub_df_test[\"Lyrics\"] = sub_df_test[\"Lyrics\"].apply(lambda x: \" \".join(word for word in x.split() if word not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "847b94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_test[\"Lyrics\"]=sub_df_test[\"Lyrics\"].apply(lambda x:(re.sub(r\"\\n\",' ',str(x))))\n",
    "sub_df_test[\"Lyrics\"]=sub_df_test[\"Lyrics\"].apply(lambda x:(re.sub( r\"[^\\w\\s]\",'',str(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "61ee6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_test[\"Lyrics\"]=sub_df_test[\"Lyrics\"].apply(lambda x: str(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "37f0da67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>most folks spend days daydreaming finding clue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Indie</td>\n",
       "      <td>take cold hands put face sharpen axe criminal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Metal</td>\n",
       "      <td>are ready time war well break fucking doors sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pop</td>\n",
       "      <td>you ask i change color hair yeah you ask i nee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>do believe magic young girls heart how music f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7930</th>\n",
       "      <td>Rock</td>\n",
       "      <td>tuesday night  730 i hear voice telephone doin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7931</th>\n",
       "      <td>Metal</td>\n",
       "      <td>elite forces cloaked fur un sensitive pain bur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7932</th>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>dr dre journey into mind maniac doomed killer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7933</th>\n",
       "      <td>Rock</td>\n",
       "      <td>you look hurricane right eye 1200 people dead ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7934</th>\n",
       "      <td>Rock</td>\n",
       "      <td>deal truth till find proof and feel weight ste...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7935 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Genre                                             Lyrics\n",
       "0     Hip-Hop  most folks spend days daydreaming finding clue...\n",
       "1       Indie  take cold hands put face sharpen axe criminal ...\n",
       "2       Metal  are ready time war well break fucking doors sm...\n",
       "3         Pop  you ask i change color hair yeah you ask i nee...\n",
       "4     Hip-Hop  do believe magic young girls heart how music f...\n",
       "...       ...                                                ...\n",
       "7930     Rock  tuesday night  730 i hear voice telephone doin...\n",
       "7931    Metal  elite forces cloaked fur un sensitive pain bur...\n",
       "7932  Hip-Hop  dr dre journey into mind maniac doomed killer ...\n",
       "7933     Rock  you look hurricane right eye 1200 people dead ...\n",
       "7934     Rock  deal truth till find proof and feel weight ste...\n",
       "\n",
       "[7935 rows x 2 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f815616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nb_words=20000\n",
    "tokenizer= Tokenizer(num_words=max_nb_words)\n",
    "tokenizer.fit_on_texts(sub_df_test.Lyrics)\n",
    "test_sequences = tokenizer.texts_to_sequences(sub_df_test.Lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a2f69f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7935, 300)\n"
     ]
    }
   ],
   "source": [
    "test_data=pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0023dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sub_df_test[\"Genre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "10b50fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Country' 'Electronic' 'Folk' 'Hip-Hop' 'Indie' 'Jazz' 'Metal' 'Pop'\n",
      " 'R&B' 'Rock']\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([ 810,  660,  495,  960,  510,  660,  810, 1110,  510, 1410],\n",
      "      dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "le= LabelEncoder()\n",
    "le.fit(labels)\n",
    "\n",
    "test_labels=le.transform(labels)\n",
    "\n",
    "print(le.classes_)\n",
    "print(np.unique(test_labels, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "29c4a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = to_categorical(np.asarray(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "90b25a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 8916,  941,  942],\n",
       "       [   0,    0,    0, ...,   46,   88,   25],\n",
       "       [   0,    0,    0, ...,  538,   10, 1625],\n",
       "       ...,\n",
       "       [ 268, 2403, 2979, ...,  152, 1715,   54],\n",
       "       [   0,    0,    0, ...,   38,  700,  785],\n",
       "       [   0,    0,    0, ...,  115,  355,  412]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2d4f9cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "274511cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_data.npy', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7f1fad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('labels_test.npy', labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e000327",
   "metadata": {},
   "source": [
    "Our 2nd best model was the Pre trained Embeddings model, so we also have apply the pretrained embeddings on the test set and save them to Google Drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ce90774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding_model = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
    "embeddings_test = embeding_model(sub_df_test.Lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f2ef064b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7935, 50), dtype=float32, numpy=\n",
       "array([[ 1.074597  , -0.5630959 , -0.63586074, ..., -0.27870524,\n",
       "         1.1905441 ,  0.56676865],\n",
       "       [ 0.8727184 , -0.20948684, -0.8824672 , ..., -0.6039505 ,\n",
       "         0.7928976 ,  0.17126192],\n",
       "       [ 0.8540335 , -0.65630573, -1.1608652 , ...,  0.10664905,\n",
       "         1.0214158 , -0.0730844 ],\n",
       "       ...,\n",
       "       [ 1.1986363 , -1.87084   , -1.2815093 , ...,  0.79785144,\n",
       "         2.1023715 ,  0.3782531 ],\n",
       "       [ 0.48767015, -0.36326748, -0.82148474, ...,  0.07695395,\n",
       "         0.5876057 , -0.02761501],\n",
       "       [ 0.49153772,  0.4132342 , -0.5963266 , ..., -0.42785105,\n",
       "         0.00440266,  0.81977504]], dtype=float32)>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "23af6c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embeddings_test.npy', embeddings_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
